---
title: "Shiny"
author: "Niko Partanen"
date: "17 Nov 2014"
---

Lately many talented linguists have asked us whether it is possible to use our corpus to study different aspects of Komi grammar. For us who compile these materials this is always a delight, as the interest tells directly that there is need for new materials.

However, I think there is still long path to go before these new electronic materials are directly usable for everyone. The main problem is not even that in order to do the coolest things one has to know quite a lot about the technical side of the things. Even with superficially simple things one has to click around like maniac and actually know the internal structure of the ELAN files in order to set the tiers correctly. So this is what I usually start to explain, and then it feels to scare away a huge number of potential users.

ELAN's search facilities are very good and Trova search engine integrated into IMDI Browser works like a charm. However, it is kind of a fact that considerable technical skills are needed even to do very basic operations. Often one would like to filter the sessions or speakers in more interesting ways than just arranging sessions into domains. Let's say you have some good metadata in XML format. How do you filter from it all speakers born at some specific decades and are males, as an example? Well, you throw in some XSLT and regular expressions, probably. R is very good tool for this sort of sophisticated corpus linguistics. But my point is that it still feels that I need to open each screw in cars engine when I just want to drive somewhere.

It's one thing to encode this kind of data to start with, and usually that's the all we documentary linguists talk about. Should we have data in this or that kind of XML structure etc etc etc. However, it still doesn't answer to the question how an ordinary linguist can easily explore the content of our recordings without getting hands dirty with the technical details.

I was playing with relatively new R package **Shiny**. The idea is that you can turn with it your R code into an online application. For many months I was thinking that it sounds way too difficult, but following the excellent instructions provided by RStudio I actually got grasp of it pretty fast. Generally speaking there are many new things worth keeping an eye on, as an example, the package **ggvis** seems to take this online publishing again to a new exciting direction.

There are lots of good people having their Shiny application code in GitHub. I was looking in detail the [R Graph Catalog](http://shinyapps.stat.ubc.ca/r-graph-catalog/) application for other reasons, as it just displayed so well so useful code for visualisations. However, very soon it just hit me that something similar could be modified into a very nice user interface for our session metadata. That's what I started to look into. And I found out that with this template it was not very difficult.

There are several things can one use to filter sessions. In one hand the parameters are **session specific**, but they can also be **participant specific**. As an example, one may want to find all sessions recorded in one region. But then one could also want to find all sessions in which there are present female speakers aged between 35-55, as an example.
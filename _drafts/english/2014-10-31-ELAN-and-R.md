---
title: "ELAN corpus and R"
author: "Niko Partanen"
date: "31 Oct 2014"
---

## Background

The corpus we are building consists from two parts. First there are the transcriptions in ELAN XML files, and the metadata is stored in IMDI or CMDI XML. Of course it is possible to say that the ELAN files themselves are a stand alone linguistic corpus, yet I think this would undermine the value of metadata in actually giving all the context into purely linguistic data that is in ELAN files.

ELAN has indeed very powerful search tools, and I think one challenge we have is that those scholars who have not been using this sort of corpus tools before should be able to start easily using the corpuses we are now compiling. Maybe the best solution at the moment is to create more down-to-earth materials about making complex corpus searches in ELAN itself. We have already started that with our colleagues in Komi Republic, Russia.

However, there are some limitations that are just inherent in many corpus tools as well as in ELAN searches. As far as I see it, we would often have a need to associate each token with some aspects of our metadata. This doesn't means to associate it with **all** metadata, but depending from our needs this could be some attributes of the speakers, location, context or genre. Occasionally we would need to slice away some parts of the data, or then hand pick only the sessions which match some complex combination of attributes.

As far as I see it, R is a tool where we can do all this relatively easily. With Shiny web framework it is also possible to wrap R code into easy and accessible web application. However, first we have to get data from ELAN files to R. For this I will introduce our new R package **FRelan**.

## ELAN and R interaction

The majority of information I've seen online about getting ELAN files into R starts with **exporting to csv, and reading this to R**. This as a problematic approach. **ELAN files are always changing.** If they are not, they certainly should be! As we work with our data we will always see typos and trivial mistakes, which we should just keep fixing as they come to our spotlight. But this doesn't really fit well with the idea of exporting from ELAN as a CSV, because then we have to create new exports every time we change something, and maintaining this routine is itself quite error-prone and laboursome once the corpus has reached more mature state.

R has good tools for parsing XML itself. However, ELAN files are relative complicated by their structure. The most advanced function in **FRelan** is called **read.eaf**. There is also a simplified version called **read.eaf.tokens**. I think they could also be in one function, but it felt easier to split it into two. The simplified version reads only the tokenized word tiers and is thereby much easier to get working than the full blown version. I'll get soon to the reasons. **read.eaf.w** takes following attributes:

read.eaf.w(path = ".", pattern = "\\.eaf$", wordLinguisticType = wordT, ignore = FALSE, ignore.pattern = "new", recursive = TRUE)

The idea is that you set you working directory to the top of your ELAN file folders. You can as well use the function from elsewhere and just give the full path to the top directory. If you don't need to dive deeper into subfolders you can set recursive pattern to FALSE.

If you have some folders or subfolders which contain structurally inconsistent ELAN files you may need to block them with arguments like **ignore = TRUE, ignore.pattern = "bad_folder"**. If your ignore argument is set to false them the ignore.pattern is not meaningful. **You probably don't know if some of your files are structurally inconsistent.**

The folder structure of our personal GitHub repository is now like this:

</br>

    - data
    |- kpv_izva
    |- kpv_udora
    |- kpv_novyje

The script runs from the highest level moving down recursively into all folders in the hierarchy. However, the files in the folder **kpv_novyje** are new ones and are not ready to be imported into corpus. This is why it gets excluded.

The script also assumes that you have some tier structure in your ELAN files. I have wrote some functions to parse non-structured ELAN files, that is pretty doable as well, but as I don't work with files like that every day I have not yet got it into FRelan package. Ideally the tier structure would be like ours in Freiburg, but it is possible and easy to define also different linguistic types and structures.

</br>

    -ref@S1					(refT)
    	-orth(orig)@S1		(orth(orig)T)
    	-orth@S1			(orthT)
    		-word@S1		(wordT)
    		-pos@S1			(posT)
    -ref@S2
    	-orth(orig)@S2
    	-orth@S2
    		-word@S2
    		-pos@S2
    	

The script in read.eaf.w is as follows:

</br>

    read.eaf.w <- function(path = ".", pattern = ".eaf$", wordLType = "wordT", ignore = "kpv_novyje", concordances = TRUE, recursive = TRUE){
    xmlfiles <- list.files(path, pattern = "*.eaf$", recursive)
    xmlfiles <- xmlfiles_all[ !grepl(ignore) ]
    n <- length(xmlfiles)
    dat <- vector("list", n)
        for(i in 1:n){
            doc <- xmlTreeParse(xmlfiles[i], useInternalNodes = TRUE)
            nodes <- getNodeSet(doc, "//TIER[@LINGUISTIC_TYPE_REF=wordLType]")
            x <- lapply(nodes, function(x){ data.frame(
            Filename = xmlfiles[i],
            Speaker = xpathSApply(x, "." , xmlGetAttr, "PARTICIPANT"),
            Token_ID = xpathSApply(x, ".//ANNOTATION/REF_ANNOTATION" , xmlGetAttr, "ANNOTATION_ID"),
            Word = xpathSApply(x, ".//ANNOTATION/REF_ANNOTATION/ANNOTATION_VALUE" , xmlValue) )})
    dat[[i]] <- do.call("rbind", x)
    }
    corpus <- do.call("rbind", dat)
    }

So basically this reads the content of the tier that has a word-level tokenisation. It picks up also the participant's name and the filename. The filename matches with the session name, of course, which makes it possible to connect the speaker and the session, which are two attributes which we need in order to connect any aspect of metadata into our tokens. It creates a data frame bit like this:

</br>

    | Filename | Speaker | Before| Token | After |
    |----------|---------|-------|-------|-------|
    | test.eaf | someone |       | hi    | there |
    | test.eaf | someone | hi    | there | !     |
    | test.eaf | someone | there | !     |       |

The script works well, but there are some issues. One is that there is much more data in ELAN file than only this! As an example, we would need the reference tiers under which each token is located! One would also like to have some information about the timing, i.e. the start and end time of the reference annotation under which the token is. Without this information we cannot really sort the tokens by the order in which they have occurred. That's why it can be advisable to use more advanced read.eaf() function. It produces a data frame like this:

</br>

    | Filename | Speaker | Token | Start | End  | Reference |
    |----------|---------|-------|-------|------|-----------|
    | test.eaf | someone | hi    | 1.12  | 2.54 | test-001  |
    | test.eaf | someone | there | 1.12  | 2.54 | test-001  |
    | test.eaf | someone | !     | 1.12  | 2.54 | test-001  |

Of course with information about all possible reference annotation id's we can connect all annotations and translations on all levels. There are several things it still demands from your ELAN files:

- Structural consistency (tiers and linguistic types have always same names)
- Tiers must have Participant attributes filled
- It's quite good idea that your filenames are same as your session names...

Let's say you get the package working, parse your ELAN files into R and start having fun with statistics. Still things often start to crash suddenly. One day it worked well, now it doesn't. That's why there is a function called:

    eaf.log(path = ".", pattern = ".eaf$, days = 3, recursive = TRUE)
    example: eaf.log(5)

Basically it reads all ELAN it finds on the path you give it and prints you list of files which have been modified within those days. This is actually really useful as it pinpoints you the files in which you may have messed something up. I screw up perfectly good files almost every day when trying to get something fixed, it happens all the time.

Another important function is:

    actors <- eaf.actors(path = ".", pattern = ".eaf$", recursive = TRUE)

It prints you a data frame with session names and the list of participants. With that it is relatively easy to check that everything is correct and also to start merging ELAN data with your metadata. Metadata comes in all forms and flavours, and it is a bit difficult to have any solution that serves everyone. There is a very primitive function that probably starts to develop once we get our IMDI files into good shape in Freiburg:

    my.metadata <- read.imdi()

Basically it reads all IMDI files that are on it's path and prints the result as a data frame. Now those IMDI files have the actors and session names, and you should be able to join the dataframes together. You can do that easily with packages like dplyr, as an example:

    left_join(actors, my.metadata
    Joining by: Speaker, Session_name

Something like this works. It's a good idea to test a bit whether all actors present on the ELAN files are also in your metadata. You may need to rename stuff a bit, both in your R data frames and in your files, but if you got this far then you probably know R enough to get onward.

Then to fancier and more experimental functions. There is one I've been working quite a lot with, but it is by far not ideal as it is now:

    write.eaf()

It is logical pair for read.eaf(). The idea is that we may want to take some text files that have no time alignation and just turn them into simple ELAN files. The reason is that this way we can easily integrate that data into our corpus linguistic toolchain and also offer it accessible through tools like Trova and ELAN search.

Functions like read.eaf() try to keep it simple. They take the ELAN data and do not manipulate it that much. Anyway, there are some really common tasks like turning word tier content into lower case Token column etc which are done now automatically. Please give us feedback and ideas if you think something could or has to be done differently. This is a work in progress!
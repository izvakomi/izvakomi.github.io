---
title: "ELAN corpus and R"
author: "Niko Partanen"
date: "31 Oct 2014"
---

## Background

The corpus we are building consists in a way from two parts. First there are the transcriptions in ELAN XML files, and the metadata is stored in IMDI or CMDI XML. Of course it is possible to say that the ELAN files themselves are a linguistic corpus, yet I think this would undermine the value of metadata in actually giving all the context into purely linguistic data that is in ELAN files.

ELAN has indeed very powerful search tools, and I think one challenge we have, I mean as documentary linguists who work with the Uralic languages, is that those scholars who have not been using this sort of corpus tools before should be able to start easily using the corpuses we are now compiling. The best solution at the moment is to create more down-to-earth kind of materials about making complex corpus searches in ELAN itself. I'm not trying to make a point that there would be something wrong in ELAN or Trova searches, they are wonderful tools.

However, there are some limitations that are just inherent in many corpus tools. As far as I see it, we would often have a need to associate each token with some aspects of our metadata. This never means to associate it with **all** metadata, but depending from our needs this could be some attributes of the speakers, location, context or genre.

As far as I see it, R is a tool where we can do all this relatively easily.

## ELAN and R interaction

The majority of information I've seen online about getting ELAN files into R starts with *exporting to csv, and reading this to R*. I've felt this as a problematic approach ever since I starter to learn R some year ago. **ELAN files are always changing.** If they are not, they certainly should be! As we work with our data we will always be seeing typos and trivial mistakes, which we should just keep fixing as they come to our spotlight. But this doesn't really fit well with the idea of exporting from ELAN as a CSV, because then we have to create new exports every time we change something, and maintaining this routine is itself quite error-prone and laboursome.

However, in his introductory book **Corpus linguistics with R**, 2009, Stefan Th. Gries writes that the corpus which is being read into R can be in different formats, among which are CSV and XML. Now as ELAN file is an XML file it should be trivial just to read that to R. However, as ELAN XML is structurally quite complex it has not been that easy.

The folder structure of our personal GitHub repository is now like this:

</br>

    - data
    |- kpv_izva
    |- kpv_udora
    |- kpv_novyje

The script runs from the highest level moving down recursively into all folders in the hierarchy. However, the files in the folder **kpv_novyje** are new ones and are not ready to be imported into corpus. This is why it gets excluded on the third line.

The script I use nowadays is as follows:

</br>

    library(XML)
    xmlfiles_all <- list.files(path=".", pattern="*.eaf$", recursive=TRUE)
    xmlfiles <- xmlfiles_all[ !grepl("kpv_novyje",xmlfiles_all) ]
    n <- length(xmlfiles)
    dat <- vector("list", n)
        for(i in 1:n){
            doc <- xmlTreeParse(xmlfiles[i], useInternalNodes = TRUE)
            nodes <- getNodeSet(doc, "//TIER[@LINGUISTIC_TYPE_REF='wordT']")
            x <- lapply(nodes, function(x){ data.frame(
            Filename = xmlfiles[i],
            Speaker = xpathSApply(x, "." , xmlGetAttr, "PARTICIPANT"),
            Word = xpathSApply(x, ".//ANNOTATION/REF_ANNOTATION/ANNOTATION_VALUE" , xmlValue) )})
    dat[[i]] <- do.call("rbind", x)
    }
    kpv_corpus <- do.call("rbind", dat)

So basically this reads the content of the tier that has a word-level tokenisation. It picks up also the participant's name and the filename. The filename matches with the session name, of course, which makes it possible to connect the speaker and the session, which are two attributes which we need in order to connect any aspect of metadata into our tokens. It creates a data frame like this:

</br>

    | Filename | Speaker | Word  |
    |----------|---------|-------|
    | test.eaf | someone | hi    |
    | test.eaf | someone | there |
    | test.eaf | someone | !     |

The script works well, but there are some issues. One is that there is much more data in ELAN file than only this! As an example, we would need the reference tiers under which each token is located! I would also like to have some information about the timing, i.e. the start and end time of the reference annotation under which the token is. Without this information we cannot really sort the tokens by the order in which they have occurred.

</br>

    | Filename | Speaker | Word  | Start | End  | Reference |
    |----------|---------|-------|-------|------|-----------|
    | test.eaf | someone | hi    | 1.12  | 2.54 | test-001  |
    | test.eaf | someone | there | 1.12  | 2.54 | test-001  |
    | test.eaf | someone | !     | 1.12  | 2.54 | test-001  |

I will be satisfied with this once we have a script that can read all the content of ELAN data into R. I figured out a way to do it by batch processing the ELAN files into a simpler XML, but I don't think that's the ideal way to go for the most of the situations. It is anyway something to consider in cases where the ELAN files are so badly formatted and structured that this R script doesn't understand them. I have an example here.

The limitation of the script now is that it really demands that you have each participant marked and that you have at least one annotation for each speaker. It makes sense, as otherwise there is nothing to match. If you have glosses it is easy to read that in with **xpathSApply**. However, I must warn that this also gets easily problematic if you don't have a matching number of annotations.

So I think the best way would be to have a script that matches tokens by their time slots. However, I have found a script like that very demanding to write.